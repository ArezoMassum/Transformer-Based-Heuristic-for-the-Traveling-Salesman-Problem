{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Packages",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import networkx as nx \n",
    "import pickle \n",
    "from networkx.algorithms.approximation import greedy_tsp # For approx TSP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-04T16:52:14.384693Z",
     "iopub.status.idle": "2024-12-04T16:52:14.385133Z",
     "shell.execute_reply.started": "2024-12-04T16:52:14.384930Z",
     "shell.execute_reply": "2024-12-04T16:52:14.384951Z"
    },
    "ExecuteTime": {
     "end_time": "2025-01-20T22:10:22.341154Z",
     "start_time": "2025-01-20T22:10:21.035101Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "## Helper functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def tour_length(G, tour):\n",
    "    \"\"\"\n",
    "    Compute the length of a tour. \n",
    "    \"\"\"\n",
    "    n = len(tour) - 1\n",
    "    assert tour[0] == tour[-1], \"Not valid tour\"\n",
    "    estimated = 0\n",
    "    for i in range(n):\n",
    "        estimated += G[tour[i]][tour[i + 1]]['weight']\n",
    "    return estimated\n",
    "\n",
    "def greedy_algorithm(G):\n",
    "    \"\"\"\n",
    "    Run the value of the greedy approximation algorithm on graph G\n",
    "    \"\"\"\n",
    "    return tour_length(G, greedy_tsp(G, weight='weight'))\n",
    "\n",
    "def random_tour(G, seed = 42):\n",
    "    \"\"\"\n",
    "    Return the value of a random tour\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n = G.number_of_nodes()\n",
    "    tour = [0]\n",
    "    for i in range(1, n):\n",
    "        next_node = np.random.choice([j for j in range(n) if j not in tour])\n",
    "        tour.append(next_node)\n",
    "    tour.append(0)\n",
    "    return tour_length(G, tour)\n",
    "    \n",
    "\n",
    "def transformer_tsp(G, model, DEVICE = 'cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate your (trained) model on G\n",
    "    \"\"\"\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Note: number of edges is constant ed equal to n(n-1)/2\n",
    "    n = G.number_of_nodes()\n",
    "    E = G.number_of_edges()\n",
    "\n",
    "    \n",
    "    # Get node coordinates\n",
    "    attr = nx.get_node_attributes(G, 'pos')\n",
    "    x = []\n",
    "    for i in range(n):\n",
    "        x.append(torch.tensor(attr[i], dtype=torch.float32))\n",
    "\n",
    "    # From list of tensors to tensor\n",
    "    x = torch.stack(x)    \n",
    "\n",
    "    tour = [0]\n",
    "    y = torch.tensor(tour, dtype=torch.long)\n",
    "    x = torch.stack(x)\n",
    "    x = x.to(DEVICE).unsqueeze(0)\n",
    "    y = y.to(DEVICE).unsqueeze(0)\n",
    "    \n",
    "    out = model(x, y)\n",
    "    \n",
    "    while len(tour) < n:\n",
    "        _, idx = torch.topk(out, n, dim=2)\n",
    "        for i in range(n):\n",
    "            if idx[0,-1, i] not in tour:\n",
    "                tour.append(idx[0, -1, i])\n",
    "                break\n",
    "        y = torch.tensor(tour)\n",
    "        y = y.to(DEVICE).unsqueeze(0)\n",
    "        out = model(x, y)\n",
    "    \n",
    "    tour = [int(i) for i in tour] + [0]\n",
    "    return tour_length(G, tour)\n",
    "\n",
    "\n",
    "\n",
    "def gap(G, model = None, model_GA = None, random_seed = 42, device = 'cpu'):\n",
    "    \"\"\"\n",
    "    Compute the gap between the optimal solution on graph G and all the analyzed methods\n",
    "    \"\"\"\n",
    "        \n",
    "    # Optimal value (hard-coded in the graph)\n",
    "    TSP = sum([G[i][j]['weight']*G[i][j]['tour'] for (i, j) in G.edges()]) # Optimal\n",
    "\n",
    "    # Gaps dictionary\n",
    "    gaps = {'greedy' : 0, 'random' : 0, 'transformer_tsp': 0, 'transformer_tsp_acc_grad': 0}\n",
    "    gaps['greedy'] = 100* (greedy_algorithm(G) -  TSP) / TSP\n",
    "    gaps['random'] = 100 * (random_tour(G, random_seed) - TSP) / TSP\n",
    "    if model is not None:\n",
    "        gaps['transformer_tsp'] = 100 * (transformer_tsp(G, model, DEVICE=device) - TSP) / TSP\n",
    "    else:\n",
    "        gaps['transformer_tsp'] = float('inf')\n",
    "        \n",
    "    if model_GA is not None:\n",
    "        gaps['transformer_tsp_acc_grad'] = 100 * (transformer_tsp(G, model_GA, DEVICE=device) - TSP) / TSP\n",
    "    else:\n",
    "        gaps['transformer_tsp_acc_grad'] = float('inf')\n",
    "    return gaps    \n",
    "    "
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-04T16:52:14.342873Z",
     "iopub.execute_input": "2024-12-04T16:52:14.343316Z",
     "iopub.status.idle": "2024-12-04T16:52:14.383799Z",
     "shell.execute_reply.started": "2024-12-04T16:52:14.343278Z",
     "shell.execute_reply": "2024-12-04T16:52:14.382051Z"
    },
    "ExecuteTime": {
     "end_time": "2025-01-20T22:10:22.424071Z",
     "start_time": "2025-01-20T22:10:22.349476Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset & Dataloader",
   "metadata": {}
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T22:10:28.525986Z",
     "start_time": "2025-01-20T22:10:22.467122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dummy dataset\n",
    "with open('dummy_20_DLL_ass4.pkl', 'rb') as file:\n",
    " dummy_data = pickle.load(file)\n",
    "\n",
    "\n",
    "single_item = dummy_data[0]\n",
    "print(single_item)\n",
    "\n",
    "\n",
    "print(f\"Type of single_item: {type(single_item)}\")\n",
    "\n",
    "# Extract the graph from the tuple\n",
    "graph = single_item[0]\n",
    "\n",
    "# Get edge attributes\n",
    "edge_attributes_weight = nx.get_edge_attributes(graph, 'weight')\n",
    "edge_attributes_tour = nx.get_edge_attributes(graph, 'tour')\n",
    "\n",
    "\n",
    "print(\"Edge attributes:\")\n",
    "print(\"- 'weight': \", edge_attributes_weight[next(iter(edge_attributes_weight))], \"(Euclidean distance between the two nodes of the edge.)\")\n",
    "\n",
    "print(\"- 'tour': \", edge_attributes_tour[next(iter(edge_attributes_tour))],\"(Binary value (0 or 1), indicating whether the edge is part of the optimal tour (1) or not (0).)\")\n",
    "\n",
    "\n",
    "# Get node attributes\n",
    "node_attributes = nx.get_node_attributes(graph, 'pos')\n",
    "\n",
    "\n",
    "print(\"Node attribute:\")\n",
    "print(\"- 'pos': \", node_attributes[0], \"representing the 2D coordinates of the node.\")\n",
    "\n",
    "class TSPDataset(Dataset):\n",
    "    def __init__(self, data, return_graph=False):\n",
    "        self.data = data\n",
    "        self.return_graph = return_graph\n",
    "\n",
    "    def __len__(self): #Return the number of items in the dataset.\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        graph, tour = self.data[idx]\n",
    "        if self.return_graph:\n",
    "            return graph, tour  # Return the original NetworkX graph\n",
    "\n",
    "        # Get the number of nodes in the graph\n",
    "        n = graph.number_of_nodes()\n",
    "\n",
    "        # Extract node coordinates (pos attribute)\n",
    "        pos = nx.get_node_attributes(graph, 'pos')\n",
    "        X = torch.tensor([[pos[i][0], pos[i][1]] for i in range(n)], dtype=torch.float32)\n",
    "\n",
    "        # Ensure the tour starts and ends with node 0\n",
    "        if tour[0] != 0:\n",
    "            tour.insert(0, 0)\n",
    "        if tour[-1] != 0:\n",
    "            tour.append(0)\n",
    "\n",
    "        # Convert the tour to a tensor\n",
    "        y = torch.tensor(tour, dtype=torch.long)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "\n",
    "# Load datasets from pickle files\n",
    "with open('train_20_DLL_ass4.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('valid_20_DLL_ass4.pkl', 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "with open('test_20_DLL_ass4.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "# Create Dataset objects using the TSPDataset class\n",
    "train_dataset = TSPDataset(train_data)\n",
    "val_dataset = TSPDataset(val_data)\n",
    "test_dataset = TSPDataset(test_data)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<networkx.classes.graph.Graph object at 0x10b71cef0>, [0, 3, 14, 2, 9, 6, 19, 13, 12, 16, 7, 18, 8, 17, 5, 11, 10, 15, 1, 4, 0])\n",
      "Type of single_item: <class 'tuple'>\n",
      "Edge attributes:\n",
      "- 'weight':  0.4287846201876535 (Euclidean distance between the two nodes of the edge.)\n",
      "- 'tour':  0 (Binary value (0 or 1), indicating whether the edge is part of the optimal tour (1) or not (0).)\n",
      "Node attribute:\n",
      "- 'pos':  (0.6049077053425551, 0.5748590937018008) representing the 2D coordinates of the node.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "## Model",
   "metadata": {}
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T22:10:28.552659Z",
     "start_time": "2025-01-20T22:10:28.544030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# --------------------------\n",
    "# Positional Encoding Class\n",
    "# --------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Encoder Class\n",
    "# --------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.input_layer = nn.Linear(2, d_model)  # Input: 2D coordinates\n",
    "\n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        # Convert input node coordinates to d_model dimensional embeddings\n",
    "        src = self.input_layer(src)\n",
    "        return self.encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Decoder Class\n",
    "# --------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, num_layers, num_nodes):\n",
    "        super(Decoder, self).__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.embedding = nn.Embedding(num_nodes, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_nodes)\n",
    "        )\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        # Convert node indices to embeddings\n",
    "        tgt = self.embedding(tgt)\n",
    "\n",
    "        # Apply positional encoding\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        # Shift the target sequence to prevent cheating\n",
    "        tgt = torch.cat((tgt.new_zeros((tgt.size(0), 1, tgt.size(2))), tgt[:, :-1, :]), dim=1)\n",
    "\n",
    "        # Pass through the decoder\n",
    "        output = self.decoder(\n",
    "            tgt, memory,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "\n",
    "        # Apply the feedforward network\n",
    "        return self.ffn(output)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# TSP Transformer Model\n",
    "# --------------------------\n",
    "class TSPTransformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, num_nodes):\n",
    "        super(TSPTransformer, self).__init__()\n",
    "        self.encoder = Encoder(d_model, nhead, dim_feedforward, dropout, num_encoder_layers)\n",
    "        self.decoder = Decoder(d_model, nhead, dim_feedforward, dropout, num_decoder_layers, num_nodes)\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        # Encode the source sequence (node coordinates)\n",
    "        memory = self.encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # Decode the target sequence (node indices)\n",
    "        output = self.decoder(tgt, memory, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Utility Function to Create Masks\n",
    "# --------------------------\n",
    "def create_mask(seq, pad_idx=0):\n",
    "    \"\"\"Create a mask for padding tokens.\"\"\"\n",
    "    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T22:10:28.586153Z",
     "start_time": "2025-01-20T22:10:28.582662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# Loss Function and Optimizer\n",
    "# --------------------------\n",
    "def get_loss_function():\n",
    "    return nn.CrossEntropyLoss()\n",
    "\n",
    "def get_optimizer(model, learning_rate=1e-4):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=1e-5)\n",
    "    return optimizer\n",
    "\n",
    "# --------------------------\n",
    "# Validation Function\n",
    "# --------------------------\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            output = model(src, tgt_input)\n",
    "            loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n"
  },
  {
   "cell_type": "markdown",
   "source": "### Training WITHOUT gradient accumulation",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# Standard Training Functions\n",
    "# --------------------------\n",
    "def train_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch using standard training.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        # Shift the target sequence\n",
    "        tgt_input = tgt[:, :-1]  # Input sequence (all except the last token)\n",
    "        tgt_output = tgt[:, 1:]  # Output sequence (all except the first token)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        output = model(src, tgt_input)  # Model prediction\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def train_model_standard(model, train_dataloader, val_dataloader, num_epochs, device):\n",
    "\n",
    "    loss_fn = get_loss_function()\n",
    "    optimizer = get_optimizer(model)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Training for one epoch\n",
    "        train_loss = train_epoch(model, train_dataloader, loss_fn, optimizer, device)\n",
    "\n",
    "        # Validation for one epoch\n",
    "        val_loss = validate_epoch(model, val_dataloader, loss_fn, device)\n",
    "\n",
    "        # Logging\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"model_standard.pth\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Main Function for Standard Training\n",
    "# --------------------------\n",
    "def main_standard_training():\n",
    "    # Device configuration\n",
    "    device = torch.device(\n",
    "        \"mps\" if torch.backends.mps.is_available() else \"cuda:0\"\n",
    "    )\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = TSPDataset(train_data)\n",
    "    val_dataset = TSPDataset(val_data)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = TSPTransformer(\n",
    "        d_model=64, nhead=4, num_encoder_layers=3, num_decoder_layers=3,\n",
    "        dim_feedforward=256, dropout=0.1, num_nodes=20\n",
    "    ).to(device)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 10\n",
    "    train_model_standard(model, train_dataloader, val_dataloader, num_epochs, device)\n",
    "\n",
    "\n",
    "# Run the standard training\n",
    "if __name__ == \"__main__\":\n",
    "    main_standard_training()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Training WITH gradient accumulation",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# Training with Gradient Accumulation Functions\n",
    "# --------------------------\n",
    "def train_epoch_with_accumulation(model, dataloader, loss_fn, optimizer, device, accumulation_steps=4):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch using gradient accumulation.\n",
    "    \"\"\"\n",
    "    model.train()  \n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()  # Clear gradients initially\n",
    "\n",
    "    for i, (src, tgt) in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        # Shift the target sequence\n",
    "        tgt_input = tgt[:, :-1]  # Input sequence (all except the last token)\n",
    "        tgt_output = tgt[:, 1:]  # Output sequence (all except the first token)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(src, tgt_input)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass (accumulate gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimizer step after accumulation_steps batches\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # Clearing gradients after step\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def train_model_with_accumulation(model, train_dataloader, val_dataloader, num_epochs, device, accumulation_steps=4):\n",
    "\n",
    "    loss_fn = get_loss_function()\n",
    "    optimizer = get_optimizer(model)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Training for one epoch with gradient accumulation\n",
    "        train_loss = train_epoch_with_accumulation(\n",
    "            model, train_dataloader, loss_fn, optimizer, device, accumulation_steps\n",
    "        )\n",
    "\n",
    "        # Validation for one epoch\n",
    "        val_loss = validate_epoch(model, val_dataloader, loss_fn, device)\n",
    "\n",
    "        # Logging\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"model_gradient_accumulation.pth\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Main Function for Gradient Accumulation Training\n",
    "# --------------------------\n",
    "def main_gradient_accumulation_training():\n",
    "    device = torch.device(\n",
    "        \"mps\" if torch.backends.mps.is_available() else \"cuda:0\"\n",
    "    )\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = TSPDataset(train_data)\n",
    "    val_dataset = TSPDataset(val_data)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = TSPTransformer(\n",
    "        d_model=64, nhead=4, num_encoder_layers=3, num_decoder_layers=3,\n",
    "        dim_feedforward=256, dropout=0.1, num_nodes=20\n",
    "    ).to(device)\n",
    "\n",
    "    # Train the model with gradient accumulation\n",
    "    num_epochs = 10\n",
    "    accumulation_steps = 4\n",
    "    train_model_with_accumulation(model, train_dataloader, val_dataloader, num_epochs, device, accumulation_steps)\n",
    "\n",
    "\n",
    "# Run the training with gradient accumulation\n",
    "if __name__ == \"__main__\":\n",
    "    main_gradient_accumulation_training()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# Testing Function\n",
    "# --------------------------\n",
    "def test_model(model, test_dataloader, device):\n",
    "    \"\"\"\n",
    "    Test the trained model on the test set and compute the optimality gap.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    gaps = {'greedy': [], 'random': [], 'transformer_tsp_standard': [], 'transformer_tsp_acc_grad': []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for graph, _ in tqdm(test_dataloader, desc=\"Testing\", leave=False):\n",
    "            # Move graph to device\n",
    "            graph = graph[0]  # Extract the graph from the dataloader\n",
    "            graph = graph.to(device)\n",
    "\n",
    "            # Compute gaps for baselines\n",
    "            gaps['greedy'].append(gap(graph, model=None)['greedy'])\n",
    "            gaps['random'].append(gap(graph, model=None)['random'])\n",
    "\n",
    "            # Compute gaps for the model trained with standard training\n",
    "            model.load_state_dict(torch.load(\"model_standard.pth\"))\n",
    "            gaps['transformer_tsp_standard'].append(gap(graph, model=model, device=device)['transformer_tsp'])\n",
    "\n",
    "            # Compute gaps for the model trained with gradient accumulation\n",
    "            model.load_state_dict(torch.load(\"model_gradient_accumulation.pth\"))\n",
    "            gaps['transformer_tsp_acc_grad'].append(gap(graph, model=model, device=device)['transformer_tsp'])\n",
    "\n",
    "    return gaps\n",
    "# --------------------------\n",
    "# Plot Gap Distributions\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "def plot_gap_distributions(gaps):\n",
    "    \"\"\"\n",
    "    Plot four boxplots showing the gap distributions for random tours,\n",
    "    the greedy algorithm, and the models (standard training and gradient accumulation).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Prepare data for boxplots\n",
    "    data = [\n",
    "        gaps['random'],\n",
    "        gaps['greedy'],\n",
    "        gaps['transformer_tsp_standard'],\n",
    "        gaps['transformer_tsp_acc_grad']\n",
    "    ]\n",
    "    labels = ['Random Tour', 'Greedy Algorithm', 'Model (Standard)', 'Model (Grad. Accum.)']\n",
    "\n",
    "    # Create boxplot\n",
    "    plt.boxplot(data, labels=labels, showmeans=True)\n",
    "    plt.title(\"Optimality Gap Distributions\")\n",
    "    plt.ylabel(\"Gap (%)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "# --------------------------\n",
    "# Main Testing Function\n",
    "# --------------------------\n",
    "def main_testing():\n",
    "    # Device configuration\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda:0\")\n",
    "\n",
    "    # Load the test dataset\n",
    "    test_dataset = TSPDataset(test_data)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # One graph at a time\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = TSPTransformer(\n",
    "        d_model=64, nhead=4, num_encoder_layers=3, num_decoder_layers=3,\n",
    "        dim_feedforward=256, dropout=0.1, num_nodes=20\n",
    "    ).to(device)\n",
    "\n",
    "    # Test the model\n",
    "    gaps = test_model(model, test_dataloader, device)\n",
    "\n",
    "    # Plot the results\n",
    "    plot_gap_distributions(gaps)\n",
    "\n",
    "\n",
    "# Run the testing\n",
    "if __name__ == \"__main__\":\n",
    "    main_testing()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}
